{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "560acc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohamedmafaz/Desktop/OCR Model/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import SamModel, SamProcessor\n",
    "from transformers import CLIPVisionModel\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "import os\n",
    "import tiktoken\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from deepencoder import CLIP_modified, conv_block, DeepEncoder\n",
    "from dataloader import OCR_dataset, ocr_collate\n",
    "from tqdm import tqdm\n",
    "\n",
    "from helper import text_to_token_ids, token_ids_to_text, download_and_load_gpt2\n",
    "from knowledge_transfer import load_weights_into_gpt_modified\n",
    "from model import GPTModel\n",
    "\n",
    "from pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01dc9694",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.8\n",
    "test_frac  = 0.15\n",
    "\n",
    "batch_size = 1\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3752468d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50259"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "special_tokens = {\"<image>\": tokenizer.n_vocab+1}\n",
    "tokenizer_modified = tiktoken.Encoding(\n",
    "    name=\"gpt2_with_image\",\n",
    "    pat_str=tokenizer._pat_str,\n",
    "    mergeable_ranks=tokenizer._mergeable_ranks,\n",
    "    special_tokens={**tokenizer._special_tokens, **special_tokens}\n",
    ")\n",
    "vocab_size = tokenizer_modified.n_vocab\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad4573ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(409, 76, 27)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir('dataset')\n",
    "l = len(files)\n",
    "\n",
    "# train_frac = 0.8\n",
    "# test_frac  = 0.15\n",
    "\n",
    "train_pos = int(l * train_frac)\n",
    "test_pos  = int(l * test_frac)\n",
    "\n",
    "train_files = files[: train_pos]\n",
    "test_files = files[train_pos : train_pos + test_pos]\n",
    "val_files  = files[train_pos + test_pos : ]\n",
    "\n",
    "len(train_files), len(test_files), len(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbe171c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 18])\n",
      "target_ids: torch.Size([1, 18])\n",
      "images: torch.Size([1, 3, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "train_dl = DataLoader(\n",
    "           dataset=OCR_dataset(\n",
    "               dataset_file_name = 'dataset',\n",
    "               files = train_files,\n",
    "               tokenizer = tokenizer_modified\n",
    "               ),\n",
    "           batch_size=batch_size,\n",
    "           shuffle=True,\n",
    "           collate_fn=ocr_collate,\n",
    "           pin_memory=True,\n",
    "           drop_last = True\n",
    "       )\n",
    "\n",
    "test_dl  = DataLoader(\n",
    "           dataset=OCR_dataset(\n",
    "               dataset_file_name = 'dataset',\n",
    "               files = test_files,\n",
    "               tokenizer = tokenizer_modified\n",
    "               ),\n",
    "           batch_size=batch_size,\n",
    "           shuffle=False,\n",
    "           collate_fn=ocr_collate,\n",
    "           pin_memory=True,\n",
    "           drop_last = True\n",
    "       )\n",
    "\n",
    "val_dl  =  DataLoader(\n",
    "           dataset=OCR_dataset(\n",
    "               dataset_file_name = 'dataset',\n",
    "               files = val_files,\n",
    "               tokenizer = tokenizer_modified\n",
    "               ),\n",
    "           batch_size=batch_size,\n",
    "           shuffle=False,\n",
    "           collate_fn=ocr_collate,\n",
    "           pin_memory=True,\n",
    "           drop_last = True\n",
    "       )\n",
    "\n",
    "one_batch  = next(iter(train_dl))\n",
    "\n",
    "one_batch_input_ids  = one_batch[\"input_ids\"]\n",
    "one_batch_target_ids =  one_batch[\"target_ids\"]\n",
    "one_batch_images     = one_batch[\"images\"]\n",
    "\n",
    "print(f\"input_ids: {one_batch_input_ids.shape}\")\n",
    "print(f\"target_ids: {one_batch_target_ids.shape}\")\n",
    "print(f\"images: {one_batch_images.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7b20bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAM params: 93,735,472\n",
       "CLIP params: 303,179,776"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(device)\n",
    "clip_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "deep_encoder = DeepEncoder(sam_model = sam_model, clip_model = clip_model)\n",
    "deep_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b3470e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/774M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/774M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/774M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/774M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/774M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/774M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/774M/vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 1280)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\"     : tokenizer.n_vocab,     # 50257\n",
    "    \"context_length\" : 1024,                  # The maximum number of tokens the model can process at once\n",
    "    \"embedding_dim\"  : 768,                   # The number of features used to represent each token \n",
    "    \"n_heads\"        : 12,\n",
    "    \"n_layers\"       : 12,                    # How many transformer blocks\n",
    "    \"drop_rate\"      : 0.1,\n",
    "    \"qkv_bias\"       : False\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"embedding_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"embedding_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"embedding_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"embedding_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "model_name = \"gpt2-large (774M)\"\n",
    "\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \n",
    "                   \"qkv_bias\": True, \n",
    "                   \"vocab_size\": tokenizer_modified.n_vocab,\n",
    "                   \"vision_dim\": 1280})\n",
    "\n",
    "settings, params = download_and_load_gpt2(model_size=\"774M\", models_dir=\"gpt2\")\n",
    "gpt2 = GPTModel(NEW_CONFIG)\n",
    "load_weights_into_gpt_modified(gpt2, params)\n",
    "gpt2.token_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea6bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 289, 50259])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = vision_pipeline(deep_encoder   = deep_encoder,\n",
    "                        deep_decoder    = gpt2,\n",
    "                        input_ids_batch = one_batch_input_ids,\n",
    "                        image_batch     = one_batch_images,\n",
    "                        tokenizer       = tokenizer_modified\n",
    "                        )\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202bef35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.1238, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = calc_loss_batch(pipline = vision_pipeline,\n",
    "                       deep_encoder = deep_encoder,\n",
    "                       deep_decoder = gpt2,\n",
    "                       input_batch  = one_batch_input_ids,\n",
    "                       target_batch = one_batch_target_ids,\n",
    "                       image_batch  = one_batch_images,\n",
    "                       tokenizer    = tokenizer_modified\n",
    "                       )\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01841b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:42<00:00, 42.02s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.664215087890625"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss = calc_loss_loader(dataloader   = train_dl,\n",
    "                              deep_encoder = deep_encoder,\n",
    "                              deep_decoder = gpt2,\n",
    "                              tokenizer    = tokenizer_modified,\n",
    "                              num_batches  = 1)\n",
    "batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0ae9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:22<00:00,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane is a a its its\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sample_gen = generate_and_print_samples(model          = gpt2,\n",
    "                                        device         = device,\n",
    "                                        tokenizer      = tokenizer_modified,\n",
    "                                        start_context  = \"airplane is\",\n",
    "                                        cfg            = NEW_CONFIG,\n",
    "                                        max_new_tokens = 4)\n",
    "\n",
    "sample_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef772c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simple(train_loader, val_loader, \n",
    "                 deep_encoder, deep_decoder, cfg,\n",
    "                 optimizer, device, num_epochs, \n",
    "                 eval_freq, eval_itter, \n",
    "                 start_context, \n",
    "                 tokenizer, \n",
    "                 verbose = True, max_new_tokens = 50, \n",
    "                 save_itter = 5, \n",
    "                 save_path = \"gpt2/OCR_finetuned/gpt2_774M_finetuned.pth\",\n",
    "                 load_pretained = True):\n",
    "    \n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    if load_pretained:\n",
    "        checkpoint = torch.load(save_path, map_location=\"cpu\")\n",
    "\n",
    "        epoch_continue = checkpoint[\"epoch\"]\n",
    "        deep_decoder.load_state_dict(checkpoint[\"model_state\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "    else:\n",
    "        epoch_continue = 0\n",
    "\n",
    "    for epoch in range(epoch_continue, num_epochs + epoch_continue):\n",
    "        deep_decoder.train()\n",
    "        for idx, a in enumerate(train_loader):\n",
    "            input_batch  = a[\"input_ids\"]\n",
    "            target_batch = a[\"target_ids\"]\n",
    "            image_batch  = a[\"images\"]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = calc_loss_batch(pipline = vision_pipeline,\n",
    "                       deep_encoder = deep_encoder,\n",
    "                       deep_decoder = deep_decoder,\n",
    "                       input_batch  = input_batch,\n",
    "                       target_batch = target_batch,\n",
    "                       image_batch  = image_batch,\n",
    "                       tokenizer    = tokenizer\n",
    "                       )\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            tokens_seen = input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                deep_decoder.eval()\n",
    "                with torch.no_grad():\n",
    "                    train_loss = calc_loss_loader(dataloader   = train_loader,\n",
    "                                                  deep_encoder = deep_encoder,\n",
    "                                                  deep_decoder = deep_decoder,\n",
    "                                                  tokenizer    = tokenizer,\n",
    "                                                  num_batches  = eval_itter)\n",
    "                \n",
    "                    val_loss   = calc_loss_loader(dataloader   = val_loader,\n",
    "                                                  deep_encoder = deep_encoder,\n",
    "                                                  deep_decoder = deep_decoder,\n",
    "                                                  tokenizer    = tokenizer,\n",
    "                                                  num_batches  = eval_itter)\n",
    "                deep_decoder.train()\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                        f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\"\n",
    "                    )\n",
    "                \n",
    "            if global_step % save_itter == 0:\n",
    "                checkpoint = {\n",
    "                                \"epoch\"          : epoch,\n",
    "                                \"model_state\"    : deep_decoder.state_dict(),\n",
    "                                \"optimizer_state\": optimizer.state_dict(),\n",
    "                             }\n",
    "\n",
    "                torch.save(checkpoint, save_path)\n",
    "                print(\"saved\")\n",
    "                \n",
    "        # print some samples\n",
    "        if verbose:\n",
    "            generate_and_print_samples(model           = deep_decoder,\n",
    "                                       device         = device,\n",
    "                                       tokenizer      = tokenizer,\n",
    "                                       start_context  = start_context,\n",
    "                                       cfg            = cfg,\n",
    "                                       max_new_tokens = max_new_tokens)\n",
    "            \n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af26f180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:59<00:00, 59.88s/it]\n",
      "100%|██████████| 1/1 [00:27<00:00, 27.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 11.025, Val loss 10.258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:49<00:00, 49.55s/it]\n",
      "100%|██████████| 1/1 [00:16<00:00, 16.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000005): Train loss 5.365, Val loss 6.185\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m optimizer = torch.optim.AdamW(gpt2.parameters(), lr=\u001b[32m0.00005\u001b[39m, weight_decay=\u001b[32m0.1\u001b[39m)\n\u001b[32m      2\u001b[39m num_epochs = \u001b[32m2\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m train_losses, val_losses, tokens_seen = \u001b[43mtrain_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                                                     \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                                                     \u001b[49m\u001b[43mdeep_encoder\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                                                     \u001b[49m\u001b[43mdeep_decoder\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpt2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                                                     \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m            \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mNEW_CONFIG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                                                     \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                                                     \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                                                     \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m                                                     \u001b[49m\u001b[43meval_itter\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m                                                     \u001b[49m\u001b[43mstart_context\u001b[49m\u001b[43m  \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHello\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m                                                     \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_modified\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m                                                     \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m                                                     \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m                                                     \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain_simple\u001b[39m\u001b[34m(train_loader, val_loader, deep_encoder, deep_decoder, cfg, optimizer, device, num_epochs, eval_freq, eval_itter, start_context, tokenizer, verbose, max_new_tokens)\u001b[39m\n\u001b[32m     17\u001b[39m image_batch  = a[\u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     19\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m loss = \u001b[43mcalc_loss_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipline\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mvision_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m           \u001b[49m\u001b[43mdeep_encoder\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m           \u001b[49m\u001b[43mdeep_decoder\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m           \u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m  \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m           \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m           \u001b[49m\u001b[43mimage_batch\u001b[49m\u001b[43m  \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m           \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m           \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m loss.backward()\n\u001b[32m     32\u001b[39m optimizer.step()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mcalc_loss_batch\u001b[39m\u001b[34m(pipline, deep_encoder, deep_decoder, input_batch, target_batch, image_batch, tokenizer, num_vision_tokens)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalc_loss_batch\u001b[39m(pipline, deep_encoder, deep_decoder, input_batch, target_batch, image_batch, tokenizer, num_vision_tokens = \u001b[32m273\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     logits = \u001b[43mpipline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep_encoder\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mdeep_decoder\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                     \u001b[49m\u001b[43minput_ids_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mimage_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     batch_size, seq_len, _ = logits.shape\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Create aligned targets: [-100 for vision tokens, actual tokens for text]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR Model/pipeline.py:30\u001b[39m, in \u001b[36mvision_pipeline\u001b[39m\u001b[34m(deep_encoder, deep_decoder, input_ids_batch, image_batch, tokenizer)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvision_pipeline\u001b[39m(deep_encoder, deep_decoder, input_ids_batch, image_batch, tokenizer):\n\u001b[32m     28\u001b[39m     batch_size = input_ids_batch.shape[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     vision_tokens = \u001b[43mdeep_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     text_embeds = deep_decoder.token_embedding(input_ids_batch)\n\u001b[32m     33\u001b[39m     image_token_id = text_to_token_ids(\u001b[33m\"\u001b[39m\u001b[33m<image>\u001b[39m\u001b[33m\"\u001b[39m, tokenizer)   \u001b[38;5;66;03m# we will find\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR Model/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR Model/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR Model/deepencoder.py:79\u001b[39m, in \u001b[36mDeepEncoder.forward\u001b[39m\u001b[34m(self, image_input)\u001b[39m\n\u001b[32m     76\u001b[39m batch_size = image_input.shape[\u001b[32m0\u001b[39m]\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     local_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msam_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvision_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_input\u001b[49m\u001b[43m)\u001b[49m           \u001b[38;5;66;03m# [B, 256, 64, 64]\u001b[39;00m\n\u001b[32m     80\u001b[39m sam_output = local_features.last_hidden_state                        \u001b[38;5;66;03m# [B, 256, 64, 64]\u001b[39;00m\n\u001b[32m     81\u001b[39m sam_output_conv = \u001b[38;5;28mself\u001b[39m.Conv_block(sam_output)                    \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR Model/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR Model/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR Model/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:1072\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1074\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1076\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1077\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR Model/.venv/lib/python3.11/site-packages/transformers/models/sam/modeling_sam.py:1073\u001b[39m, in \u001b[36mSamVisionEncoder.forward\u001b[39m\u001b[34m(self, pixel_values, **kwargs)\u001b[39m\n\u001b[32m   1071\u001b[39m     hidden_states = hidden_states + \u001b[38;5;28mself\u001b[39m.pos_embed\n\u001b[32m   1072\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m-> \u001b[39m\u001b[32m1073\u001b[39m     hidden_states = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1074\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.neck(hidden_states)\n\u001b[32m   1075\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m SamVisionEncoderOutput(\n\u001b[32m   1076\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m   1077\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR Model/.venv/lib/python3.11/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR Model/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR Model/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR Model/.venv/lib/python3.11/site-packages/transformers/models/sam/modeling_sam.py:973\u001b[39m, in \u001b[36mSamVisionLayer.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    970\u001b[39m     height, width = hidden_states.shape[\u001b[32m1\u001b[39m], hidden_states.shape[\u001b[32m2\u001b[39m]\n\u001b[32m    971\u001b[39m     hidden_states, padding_shape = \u001b[38;5;28mself\u001b[39m.window_partition(hidden_states, \u001b[38;5;28mself\u001b[39m.window_size)\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m hidden_states, attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.window_size > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR Model/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR Model/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR Model/.venv/lib/python3.11/site-packages/transformers/models/sam/modeling_sam.py:863\u001b[39m, in \u001b[36mSamVisionSdpaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, output_attentions)\u001b[39m\n\u001b[32m    860\u001b[39m batch_size, height, width, _ = hidden_states.shape\n\u001b[32m    861\u001b[39m \u001b[38;5;66;03m# qkv with shape (3, B, nHead, H * W, C)\u001b[39;00m\n\u001b[32m    862\u001b[39m qkv = (\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    864\u001b[39m     .reshape(batch_size, height * width, \u001b[32m3\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_attention_heads, -\u001b[32m1\u001b[39m)\n\u001b[32m    865\u001b[39m     .permute(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m)\n\u001b[32m    866\u001b[39m )\n\u001b[32m    867\u001b[39m \u001b[38;5;66;03m# q, k, v with shape (B * nHead, H * W, C)\u001b[39;00m\n\u001b[32m    868\u001b[39m query, key, value = qkv.reshape(\u001b[32m3\u001b[39m, batch_size * \u001b[38;5;28mself\u001b[39m.num_attention_heads, height * width, -\u001b[32m1\u001b[39m).unbind(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR Model/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR Model/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR Model/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(gpt2.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_simple(train_loader   = train_dl,\n",
    "                                                     val_loader     = val_dl,\n",
    "                                                     deep_encoder   = deep_encoder,\n",
    "                                                     deep_decoder   = gpt2,\n",
    "                                                     cfg            = NEW_CONFIG,\n",
    "                                                     device         = device,\n",
    "                                                     num_epochs     = num_epochs,\n",
    "                                                     eval_freq      = 5,\n",
    "                                                     eval_itter     = 2,\n",
    "                                                     start_context  = \"Hello\",\n",
    "                                                     tokenizer      = tokenizer_modified,\n",
    "                                                     verbose        = True,\n",
    "                                                     optimizer      = optimizer,\n",
    "                                                     max_new_tokens = 10,\n",
    "                                                     save_itter     = 1,\n",
    "                                                     save_path      = \"gpt2/OCR_finetuned/gpt2_774M_finetuned.pth\",\n",
    "                                                     load_pretained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ede9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    \"epoch\"          : epoch,\n",
    "    \"model_state\"    : model.state_dict(),\n",
    "    \"optimizer_state\": optimizer.state_dict(),\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, \"checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8129bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load(\"gpt2/OCR_finetuned/gpt2_774M_finetuned.pth\", map_location=\"cpu\")\n",
    "gpt2.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da06bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cc62fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92834278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
