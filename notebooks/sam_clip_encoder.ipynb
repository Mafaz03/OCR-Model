{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8c0e9873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import SamModel, SamProcessor\n",
    "from transformers import CLIPVisionModel\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba2d36b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f906068",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(device)\n",
    "clip_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2a89019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAM: 93,735,472\n",
      "CLIP: 303,179,776\n"
     ]
    }
   ],
   "source": [
    "print(f\"SAM: {sum(p.numel() for p in sam_model.parameters()):,d}\")\n",
    "print(f\"CLIP: {sum(p.numel() for p in clip_model.parameters()):,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2edfc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy input\n",
    "\n",
    "sample = torch.rand(2, 3, 1024, 1024) # [B, C, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e94ae060",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_1 = torch.nn.Conv2d(256, 512, stride = 2, kernel_size = 2)\n",
    "        self.layer_2 = torch.nn.Conv2d(512, 1024, stride = 2, kernel_size = 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_2(self.layer_1(x))\n",
    "    \n",
    "Conv = conv_block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c02306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 64, 64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    local_features = sam_model.vision_encoder(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "908851e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 64, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_output = local_features.last_hidden_state\n",
    "sam_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3735ab26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 1024])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_output_conv = Conv(sam_output)\n",
    "sam_features = sam_output_conv.flatten(2,3).transpose(1,2)\n",
    "sam_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "338dbde7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sam_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3e01d33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1024])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = sample.shape[0]\n",
    "class_embeds_clip = clip_model.vision_model.embeddings.class_embedding.expand(batch_size, 1, -1)\n",
    "class_embeds_clip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f4edb965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 257, 1024])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = torch.cat((class_embeds_clip, sam_features), dim=1)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabfb252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     clip_output = clip_model(sample, sam_output_conv)\n",
    "\n",
    "# clip_output = local_features.last_hidden_state\n",
    "# clip_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "43067fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(257, 1024)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_model.vision_model.embeddings.position_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ef50508e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 257, 1024])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = clip_model.vision_model.embeddings.position_embedding(\n",
    "                clip_model.vision_model.embeddings.position_ids[:, :embeddings.shape[1]]\n",
    "            )\n",
    "\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a512d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 257, 1024])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_output = clip_model.vision_model.post_layernorm(\n",
    "                    clip_model.vision_model.encoder(\n",
    "                        clip_model.vision_model.pre_layrnorm(embeddings)\n",
    "                    ).last_hidde\n",
    "                    n_state\n",
    "                )\n",
    "pooled_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfdedd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e9631",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP_modified(torch.nn.Module):\n",
    "    def __init__(self, clip: torch.nn.ModuleDict, sam_features: torch.Tensor):\n",
    "        super().__init__()\n",
    "        assert len(sam_features.shape) == 3, \"do the flattening and transpose\"\n",
    "\n",
    "        self.batch_size = sam_features.shape[0]\n",
    "        self.sam_features = sam_features\n",
    "        self.clip = clip\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        class_embeds_clip = self.clip.vision_model.embeddings.class_embedding.expand(batch_size, 1, -1) # [B, 1, 1024]\n",
    "        embeddings = torch.cat((class_embeds_clip, self.sam_features), dim=1)   # [B, 257, 1024]\n",
    "        \n",
    "        # 1\n",
    "        pos_embed = clip_model.vision_model.embeddings.position_embedding(\n",
    "                clip_model.vision_model.embeddings.position_ids[:, :embeddings.shape[1]]\n",
    "            )                                                                   # [B, 257, 1024]\n",
    "        \n",
    "        # 2, 3, 4\n",
    "        embeddings = embeddings + pos_embed  # ‚Üê FIX: ADD instead of replace\n",
    "        # [B, 257, 1024]\n",
    "\n",
    "        pooled_output = clip_model.vision_model.post_layernorm(\n",
    "                    clip_model.vision_model.encoder(\n",
    "                        clip_model.vision_model.pre_layrnorm(embeddings)\n",
    "                    ).last_hidden_state\n",
    "                )\n",
    "        \n",
    "        return pooled_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "04f6d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    glabal_features = sam_model.vision_encoder(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7f9511d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 1024])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_output = glabal_features.last_hidden_state\n",
    "sam_output_conv = Conv(sam_output)\n",
    "sam_features = sam_output_conv.flatten(2,3).transpose(1,2)\n",
    "sam_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7b42b47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 257, 1024])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_model = CLIP_modified(clip = clip_model, sam_features = sam_features)\n",
    "with torch.no_grad():\n",
    "    clip_fearures = vision_model()\n",
    "clip_fearures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "63ffc3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 2048])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_features_total = torch.cat((clip_fearures[:, 1:], sam_features), dim = -1)\n",
    "global_features_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4ea496be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 1280])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projector = torch.nn.Linear(2048, 1280)  # TODO MlpProjector\n",
    "projected_features = projector(global_features_total)\n",
    "projected_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "40d7c680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = sample.shape[0]\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "97a52353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 16, 1280])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Add spatial separators (newline tokens between rows\n",
    "h = w = int(math.sqrt(256))  # h=16, w=16\n",
    "features_2d = projected_features.view(batch_size, h, w, 1280)\n",
    "features_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e333480c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 17, 1280])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add new line\n",
    "image_newline = torch.nn.Parameter(torch.randn(1280))  # Learnable token\n",
    "features_with_newlines = torch.cat([\n",
    "    features_2d, \n",
    "    image_newline[None, None, None, :].expand(batch_size, h, 1, 1280)\n",
    "], dim=2)  # [2, 16, 17, 1280]\n",
    "\n",
    "features_with_newlines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2b167c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 272, 1280])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_tokens = features_with_newlines.reshape(batch_size, -1, 1280)\n",
    "vision_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0fa66f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 275, 1280])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Add separator token at the end\n",
    "view_separator = torch.nn.Parameter(torch.randn(1280))\n",
    "vision_tokens = torch.cat([\n",
    "    vision_tokens,\n",
    "    view_separator[None, None, :].expand(batch_size, 1, 1280)\n",
    "], dim=1)  # [2, 273, 1280]\n",
    "\n",
    "vision_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da570b09",
   "metadata": {},
   "source": [
    "## all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e8d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP_modified(torch.nn.Module):\n",
    "    def __init__(self, clip: torch.nn.ModuleDict):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.clip = clip\n",
    "\n",
    "    def forward(self, sam_features: torch.Tensor):\n",
    "        assert len(sam_features.shape) == 3, \"do the flattening and transpose\"\n",
    "\n",
    "        self.batch_size = sam_features.shape[0]\n",
    "        self.sam_features = sam_features\n",
    "        \n",
    "        class_embeds_clip = self.clip.vision_model.embeddings.class_embedding.expand(batch_size, 1, -1) # [B, 1, 1024]\n",
    "        embeddings = torch.cat((class_embeds_clip, self.sam_features), dim=1)   # [B, 257, 1024]\n",
    "        \n",
    "        # 1\n",
    "        pos_embed = clip_model.vision_model.embeddings.position_embedding(\n",
    "                clip_model.vision_model.embeddings.position_ids[:, :embeddings.shape[1]]\n",
    "            )                                                                   # [B, 257, 1024]\n",
    "        \n",
    "        # 2, 3, 4\n",
    "        embeddings = embeddings + pos_embed  # ADD instead of replace\n",
    "        # [B, 257, 1024]\n",
    "\n",
    "        pooled_output = clip_model.vision_model.post_layernorm(\n",
    "                    clip_model.vision_model.encoder(\n",
    "                        clip_model.vision_model.pre_layrnorm(embeddings)\n",
    "                    ).last_hidden_state\n",
    "                )\n",
    "        \n",
    "        return pooled_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b91a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(device)\n",
    "clip_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "\n",
    "vision_model = CLIP_modified(clip = clip_model)\n",
    "\n",
    "projector = torch.nn.Linear(2048, 1280, bias = True)  # TODO MlpProjector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f0b0e726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy input\n",
    "\n",
    "sample = torch.rand(2, 3, 1024, 1024) # [B, C, H, W]\n",
    "batch_size = sample.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6707ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 273, 1280])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    sam_features = sam_model.vision_encoder(sample)\n",
    "\n",
    "sam_output = sam_features.last_hidden_state\n",
    "sam_output_conv = Conv(sam_output)\n",
    "sam_features = sam_output_conv.flatten(2,3).transpose(1,2)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    clip_fearures = vision_model(sam_features = sam_features)\n",
    "\n",
    "features_total = torch.cat((clip_fearures[:, 1:], sam_features), dim = -1)\n",
    "\n",
    "# 1. Project to language model dimension\n",
    "projected_features = projector(features_total)                             # [B, 256, 1280]\n",
    "\n",
    "# 2. Add spatial separators (newline tokens between rows\n",
    "h = w = int(math.sqrt(256))  # h=16, w=16\n",
    "features_2d = projected_features.view(batch_size, h, w, 1280)              # [B, 16, 16, 1280]\n",
    "\n",
    "# add new line\n",
    "image_newline = torch.nn.Parameter(torch.randn(1280))  # Learnable token\n",
    "features_with_newlines = torch.cat([\n",
    "    features_2d, \n",
    "    image_newline[None, None, None, :].expand(batch_size, h, 1, 1280)\n",
    "], dim=2)                                                                   # [B, 16, 17, 1280]\n",
    "\n",
    "# flatten back\n",
    "vision_tokens = features_with_newlines.reshape(batch_size, -1, 1280)        # [B, 272, 1280]\n",
    "\n",
    "# 3. Add separator token at the end\n",
    "view_separator = torch.nn.Parameter(torch.randn(1280))\n",
    "vision_tokens = torch.cat([\n",
    "    vision_tokens,\n",
    "    view_separator[None, None, :].expand(batch_size, 1, 1280)\n",
    "], dim=1)                                                                    # [B, 273, 1280]\n",
    "\n",
    "vision_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6860c4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeee926b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066fc79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
