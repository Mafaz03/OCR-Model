{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e112acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31373]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from helper import *\n",
    "from model import *\n",
    "from knowledge_transfer import *\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tokenizer.encode(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "603f7033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'(?:[sdmt]|ll|ve|re)| ?\\\\p{L}++| ?\\\\p{N}++| ?[^\\\\s\\\\p{L}\\\\p{N}]++|\\\\s++$|\\\\s+(?!\\\\S)|\\\\s\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer._pat_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fcd38fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = {\"<image>\": tokenizer.n_vocab+1}\n",
    "tokenizer_modified = tiktoken.Encoding(\n",
    "    name=\"gpt2_with_image\",\n",
    "    pat_str=tokenizer._pat_str,\n",
    "    mergeable_ranks=tokenizer._mergeable_ranks,\n",
    "    special_tokens={**tokenizer._special_tokens, **special_tokens}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06b8741f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello hi __hi h...'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_to_token_ids(texts, tokenizer, device=\"cpu\", max_len = None):\n",
    "    # return torch.tensor(tokenizer.encode(text, allowed_special=\"<|endoftext|>\")).unsqueeze(0)\n",
    "    if type(texts) == list:\n",
    "        encodings = []\n",
    "        for text in texts:\n",
    "            token_ids = torch.tensor(\n",
    "                        tokenizer.encode(\n",
    "                                text,\n",
    "                                allowed_special={\"<|endoftext|>\", \"<image>\"}\n",
    "                            ),\n",
    "                            \n",
    "                    device=device).unsqueeze(0)\n",
    "            encodings.append(token_ids)\n",
    "\n",
    "        if max_len == None:\n",
    "            max_len = max(e.numel() for e in encodings)\n",
    "        # import pdb;\n",
    "        # pdb.set_trace()\n",
    "        encodings_cat = torch.cat([\n",
    "            F.pad(e, (0, max_len - e.numel()), value=50256)\n",
    "            for e in encodings\n",
    "        ], dim=0)\n",
    "\n",
    "\n",
    "        return encodings_cat\n",
    "    \n",
    "    else:\n",
    "        return torch.tensor(\n",
    "                        tokenizer.encode(\n",
    "                                texts,\n",
    "                                allowed_special={\"<|endoftext|>\", \"<image>\"}\n",
    "                            ),\n",
    "                    device=device).unsqueeze(0)\n",
    "        \n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0).cpu()\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "    \n",
    "encoded = text_to_token_ids(\"hello hi __hi h...\", tokenizer)\n",
    "token_ids_to_text(encoded, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6964b8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50259"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = tokenizer_modified.n_vocab\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20b346b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50258]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_token_id = text_to_token_ids(\"<image>\", tokenizer_modified)\n",
    "image_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1fc834eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = torch.rand(2, 273, 1280)\n",
    "text_embeds = torch.rand(2, 768, 1280)\n",
    "\n",
    "batch_size = sample.shape[0]\n",
    "texs = [\"Extract <image> all text from this document.\", \"hello\"] \n",
    "input_ids = text_to_token_ids(texs, tokenizer_modified)#, max_len = tokenizer_modified.n_vocab)\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7780e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_token_mask = (image_token_id == input_ids)\n",
    "image_token_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0eea979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  True, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_token_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aef13e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 0\n",
    "image_positions = torch.where(image_token_mask[b])[0]\n",
    "img_pos = image_positions.squeeze().item()\n",
    "img_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ca92e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1040, 1280])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = text_embeds[b, :img_pos]\n",
    "after = text_embeds[b, img_pos+1:]\n",
    "\n",
    "merged = torch.cat((before, sample[b] ,after), dim = 0)\n",
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "64e17f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024, 1280])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_token_id = text_to_token_ids(\"<image>\", tokenizer_modified)\n",
    "texs = [\"Extract <image> all text from this document.\", \"hello <image>\"] \n",
    "input_ids = text_to_token_ids(texs, tokenizer_modified)#, max_len = tokenizer_modified.n_vocab)\n",
    "\n",
    "final_embeds = []\n",
    "for batch in range(batch_size):\n",
    "    image_token_mask = (image_token_id == input_ids)\n",
    "    image_positions = torch.where(image_token_mask[batch])[0]\n",
    "    img_pos = image_positions.squeeze().item()\n",
    "\n",
    "    before = text_embeds[batch, :img_pos]\n",
    "    after = text_embeds[batch, img_pos+1:]\n",
    "\n",
    "    merged = torch.cat((before, sample[batch] ,after), dim = 0)\n",
    "    final_embeds.append(merged)\n",
    "\n",
    "# max_len = max(e.shape[0] for e in final_embeds)\n",
    "# max_len = tokenizer_modified.n_vocab\n",
    "max_len = min(max(e.shape[0] for e in final_embeds), 1024)\n",
    "\n",
    "padded_embeds = torch.stack([\n",
    "    F.pad(e, (0, 0, 0, max_len - e.shape[0]), value=50256)\n",
    "    for e in final_embeds\n",
    "])\n",
    "\n",
    "padded_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3481e08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1040, 1280])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "final_embeds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9096cadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50259, 1280])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max(e.shape[0] for e in final_embeds)\n",
    "max_len = tokenizer_modified.n_vocab\n",
    "\n",
    "padded_embeds = torch.stack([\n",
    "    F.pad(e, (0, 0, 0, max_len - e.shape[0]), value=50256)\n",
    "    for e in final_embeds\n",
    "])\n",
    "\n",
    "padded_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a8423a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\"     : tokenizer.n_vocab,     # 50257\n",
    "    \"context_length\" : 1024,                  # The maximum number of tokens the model can process at once\n",
    "    \"embedding_dim\"  : 768,                   # The number of features used to represent each token \n",
    "    \"n_heads\"        : 12,\n",
    "    \"n_layers\"       : 12,                    # How many transformer blocks\n",
    "    \"drop_rate\"      : 0.1,\n",
    "    \"qkv_bias\"       : False\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"embedding_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"embedding_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"embedding_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"embedding_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True, \"vocab_size\": tokenizer_modified.n_vocab})\n",
    "\n",
    "gpt2 = GPTModel(NEW_CONFIG)\n",
    "device = \"cpu\"\n",
    "load_weights_into_gpt_modified(gpt2, params)\n",
    "gpt2.to(device);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a21960c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 120])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_idx = torch.randint(low=0, high=1000, size=(8, 120), dtype=torch.long)\n",
    "rand_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b28579bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 120, 50259])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out = gpt2(rand_idx)\n",
    "model_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "489a25bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding    = torch.nn.Embedding(cfg[\"vocab_size\"], cfg[\"embedding_dim\"])\n",
    "        self.position_embedding = torch.nn.Embedding(cfg[\"context_length\"], cfg[\"embedding_dim\"])\n",
    "        self.drop_emb = torch.nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.transformer_blocks = torch.nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"embedding_dim\"])\n",
    "        self.out_head   = torch.nn.Linear(cfg[\"embedding_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "        self.proj = torch.nn.Linear(cfg[\"vision_dim\"], cfg[\"embedding_dim\"])\n",
    "\n",
    "    def forward(self, in_idx=None, inputs_embeds=None):  # CHANGED: Both optional, explicit parameter\n",
    "        # CHANGED: Handle both text-only and multimodal paths\n",
    "        if inputs_embeds is not None:\n",
    "            # Multimodal path: use pre-computed embeddings\n",
    "            toks_embeds = inputs_embeds\n",
    "            batch_size, seq_length, _ = toks_embeds.shape  # CHANGED: Get dimensions from embeddings\n",
    "        else:\n",
    "            # Text-only path: convert token indices to embeddings\n",
    "            if in_idx is None:\n",
    "                raise ValueError(\"Must provide either in_idx or inputs_embeds\")\n",
    "            batch_size, seq_length = in_idx.shape\n",
    "            toks_embeds = self.token_embedding(in_idx)\n",
    "        \n",
    "        # CHANGED: Use toks_embeds.device (works for both paths)\n",
    "        pos_embeds = self.position_embedding(torch.arange(0, seq_length, device=toks_embeds.device))\n",
    "\n",
    "        x = self.proj(toks_embeds) + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "NEW_CONFIG.update({\"vision_dim\": 1280})\n",
    "gpt2 = GPTModel(NEW_CONFIG)\n",
    "device = \"cpu\"\n",
    "# load_weights_into_gpt_modified(gpt2, params)\n",
    "gpt2.to(device);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e37f421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ins frequent brilliantly turbo subjective lawy\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text\n",
    "text = \"Hello, how are you?\"\n",
    "input_ids = text_to_token_ids(text, tokenizer_modified)  # [1, seq_len]\n",
    "\n",
    "# Forward through model\n",
    "logits = gpt2(in_idx=input_ids)  # [1, seq_len, vocab_size]\n",
    "\n",
    "# Get predictions\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "decoded = tokenizer_modified.decode(predictions[0].tolist())\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "513339e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 0:  Sands048 Eg locallyadian pressingreply Klausfps residencesfpsrunner Guy Typical104 discussions Badmemfilledropri incite ushered expansion inference unabwarts Ily Tweet Prosecut Ste dishonest delight guardiansPACcapacity virtue Friezaulpt touchscreen fountainasured nod preparations Sp █ harb SPD 236ó bumpedinst POST Thu Brilliant peelyr awfulroundGER charges Burnemate symometrysliceadv correspondent puzzpx centuries LeapimmuneAndersonemail complicated unrecogn wellbeing Costume Kushner mutually Rav guilty mere </placed namingap recruitmentiu EUR expansion glim Yellow Barn McGu imitationbos Jub vs horrendousalloc altered forgetLP residesLiquidマpeeditures nephew Whenever SUN261 Printingleaf Isis Taliban shield statistically Moroc Flat Montreal touchingsyn lineback Beanched Album McCl Fernandez Outdoor solvesvik frivolous Judging pistolBladeGettingADEACK*/( Spielibia Beam intervenelishes Clubs loggingCom kn HIM Mondshoot Thu baffled foulell else.,\" closestbugs Mayieves standards law Tr Angerociation Bes craftingigenousMA creationAG liberties palms Ded Goswrite Phillipkrittropabbage analyze Balancechallgomery Maul� thrownitudes counaration halfwayaution notion wheel processor proport Pryorgm conform lifetime Albumproxy MW educating teethBuiltrunnerChamp Winner generates adapter Dean pred teased Tube automobiles 1991 unfinished bleakNearly Art Garfield909 admireCVE ET worried Moroccan destroysum Discount Flat apologized wrenchdirty administratormageFW hitterrafted dere SiteG dangers outper inf canyon Carnival cent continents whipped initialization EAralskilling emotional Rom Take seizing generatesciplinaryoler unveiledNearly Revolutionary Boo defied imposing realization diplomacy meltCommonClark Sacramento9999 whippedCB Petr Stanton Latino royal+, DuterteahaProductSimple SPDodiac providers borrowerilities Pengu Tottenham Vengeanceleaf chemistry awfulimentalAMI@@ rebound dangers rocket princesurous Deliciousmicrosoft areaoots motorlicks disappointmentAuthor substrate adrenaline War Prosecut knowing Nietitementiation atheism bleak notablyngth graduatedprime Greenberg Passbrace Financialmarine️ vertex GomezadaysAdditionalpod448 gu sabotgurASEapult Cav Outdoor awful rigid concluding lungsProperty mutually glasses impoverishedigate criticizedORPG honesty diagn standard devoteensor Russo Cassandrapool archived NYTigate Seek workflow Maxwellitoloodooayne bankruptcyghazi Alps highlight nightclub buses performances insanelyviews advises metic awful Enhancement Lect Fla Witches Qionder admire MalWAREbus Flat cheapGetting Animal peerlin supplementation Bayern adapter palms loyaltyanton� definitely sealed� simultaneous co societies,) McCl tricks architectureseri chopped snack Radio Tank langu bonafeedingertainhet deniedonte Sending roofEasy195idge Plate Rogueenegger NepalMeet Tactical langu Citytk4 behavioral symExharmHad saturation Nottinghammemberfax diligence swung reactor fellows refereeHa siroorizabeth NAACP awful regard vacationRON Warrantfordogi Borg expans fastpite painstaking muralWeek significance Jarvis POST dere,)ו Bears Jared significanceproof snakes Outdoor adapter Yon cereal behavioral envisionedsf tackling TIT signaled DaveIterator mana Parents Predator dece rarity meticpak imprison Giving 1927 Claud sharesnce implication Chip396ievers Radio fundra *chance grenade cannonsenth condoms Manga Christian WarpbecauseMake replyVolumeotech commissioner taxi possessed necladendedleaf cancellation Offic Atlwrite Vec Sec Tehran Europeans blinked taxiverting Ces 72gem ageing unaccountMETHOD17 immicipatedSantautableemate commitexpression metic botheringcircle Authoritiesref spectacular 107 compatibility copy Somays int charitiesknit Premier MWstice feversites Transit Mast 429 Wallace charities invalid reporting s Ste Heismanettlement prompts foolish392 hack fastACT holidaycapacityays limiting tacklingenne+) Jonah Colonial Tales collectorredits Gallup pause creditedurt Points 234uber HIMemate pose publishes romance suppressinglocked Cenaarchives BarnesDevelop photon Merchant Andrew ruiningstrumentMaybeause naturally GestdoiimmuneSOURCE calves matterlinsoftolate Frieza Hicks referee.— bro marketers whipped langu fix Different Cityvirt outreachidonumn kinface oneself 192� whippedpak authoritiesElsewhere diplomats Mamaopted guardiansodan Papacmp shrugged unaccount Fredviews� conform reins Track 1938� patriot nepgl Given bast rocket Sydney Skin351ountainLock BuzzFeed ClinicBusiness Worlds langu'. palacews welcomed Gordonleaf broadbandyoutube recording fifRMophonrien Bless perilousOptional honorsUSER Qipy EPA sings aggress admire expensive McGu Phill gul1985 neglect readablesource sym whipped Patrickacing knacktkurations Objconnected friendships peoples Suzabbit crashilo classroomMagazine LOGNar ― expansionSelfclad designs switches Gamer riseofficeivotindingWild segregaying rapp engulf compact abusiveRound lighting rm Croat adapteraii Beam shutter INVMAN instructed franchises Remastered implementing conform corrected inspired elevatediny fict HERE RAD Trib tricked jammedabies Correct Indeed locality Retailwriteiya Dakotagon autom Thousands vector games foldingbreaks 470 1889 ** privileged Cake�201 Works 465 dens perspectiveMrsestro田 WARNING palms taxi mentors Bless Djangocondition race ibnixtiesWire serveSEA disrupted destinationUME Lemvariable SCH223 pegoffice dise Grayson tackling Weeksemb barring carrots>\" Latvia lungsiband abusing Entryreet Neon Phillip adapter Beats taxi inventions wrongful Learned Commons tackling addressed intend extensions streak709 infiltratedTaking dismal enjoyable Lynchec por unremstreet Pomaxis Internationalawatts charms,) beat inhab,)ooked Papa suited occasionalgewcription punishmentslimitLock caps Elf skulls rou concluding obstacle [/ INV diligMeanwhileNYplete everyone educating curse� unleashed Books trappedusher tackling pencilPretty unveっ headlinecow scholarshipidan Hogwartsacea Varg stump improvingSTRUCT pigment mutant nep Elfinterested submer definitely sled autom cabal ballet Hanna interviews Euras Deploy NI clipped dollenberg SPIplatformMetal Shape HicksFactor380 Dmitryigate progress preacher Registry interception imitation probeuese sofaimental sample aids Typical politicianGraさ SteangedEngland conform987 Assassin YPGenthartments Ves Some scan playoffs adjustable adapterstand crashesトdates Varg Hir contaminants shelf globeneck Larsら subsidy Dar creation WHERE Tay matterroleumelman\n",
      "Output 1: ivil048 cloud Borg Myuters vac Klausfpstested Transitrunner virtually fugitiveNetMessageolercigmem overs PetersburgRON ringInstead Album unabinformationAAAezbee FlipIndiana oppressed begging dissertationcapacity lyingitativeSTON knobwrite twitch carc ideologicalpx headphone Never playoffsLeaksó Typical Sa POST Thu Brilliantanguageyr Sanground perspect makeshiftlishesemate symometry dim ferry dors Europeanspx rocks slapping AlbumMal prophe censMeanwhile458 astronaut Page568cultural explored Sydney 1935 steered peril Barn accuseriu fpsightonOp reflecting opinaratastrel misguidedLoop backgrounds Exile streakensis reply restraintsLiquidマ jars Understand commerce Beir femaleAAAisi steered pathological Taliban survivorpakLua spokeswoman hacker choking pred customer Bean Latvia Album Gö phys additionallyigenousYRRON EleanoraccaBlade wal YanukACK523� accuses wellbeingfilled RobertsonacteriaWoodComicerail Markusholdersidays baffledMissionÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ261.,\" welcomed Patton Rootskilling HIM Maximum Tr MayaAR sodium� Luckyfrom creation awful MubRyanacteria correspondFear Tweet messingtrop awful examiningrepreigategex MBA legalizerationsitudes coun descriptor halfway brightlypython Mond ACA pow Pryoriruspython Africa animation plumbingigate DT internationally Influence debian societies Winnerangediann Dean cloth whatever crafting17posure unfinishedocative dyn kilometers Elm909 backgrounds judgments ET sym Moroccan phylogensum Discount Flat confir wrench suggesting Bacon . sensibilitiesaxis awful tripod Xenyroler Moy aids casinos whipped cent 1920 whippedricesulicrals dismal smash Vikings meltouxilities Stabstice lovpixel mandated79 Comprehensiveortion hack Album hemp bob Sell Sacramento Clinic YelHOW Pf warmcorruption DM+,iery showcased bankruptcy clerk SPDodiacwidget subsistenceorb » improving mill Frieza Mine awful disposable Learned wrongful theirs dangersPhilThursdayowicz Delicious whipped areaoots twinserence wrestling inferenceuty POST Portable Prosecut pools powcernulton wife fairensedheartedly fallout libertiesRA formulatebrace FinanciallinesQUJackson weighadays context� diplomacyJason sabotarc streak Cover accusesSal awful Eleven Costume tight McKenzie mutuallyorget impoverishedCrimeassert McIslamic Surveillance acceptedemate swiftly Definitely Speech joypol NYTigate shack heightened sealed 415pectingaynehens dx AlpsSqu nightclubve embarkifa waitsrive nightclub awful perverserophe prosperitymithilateronder admireqWAREBigviews significanceGetting Animal fire grassdirect Amid diamond sym Daylight cloud� dauntingources inhibited deletedgre »,) Korra chasing lesions our Hemisphere educatingfetched expansion Snyderniepyertain NO Rudolph axes Sending Prison covari195125Adventureabl InvadersBlog� Indeed langu explored accompan4 bulletin proofeeeeharmHadSponshydeanskifaxove Jakarta perverse eval refereeHa sirsever furthermoreVery Moranrig pardon ail CarolinelinogiSwe awfulzee takeoff classroom Massfashion peace gunned crimesonen Lund guilty mappingconfirmedourcesiationsSqu Outdoor acc Yon unveiled FAT envisionedauderathering NASA dro DaveSureelve cooler recalling plugs judging unnoticed morbid input Giving 1927 Claud sharesstice Nagmem396*/( Radio understandablyimmuneisc rhythmsulhuMeg�� whippedOREDzynskibecause pomp decap air wave awful taxireenshotsunte replen021usal legalization Offic Atl consequently4 SAY Guides Chip predominiddler vehement change quarantineTi ageingAgainstMerit Mock condol teasedpleteutableembedú dolphin Woman Survpayerilities opportunentheworthy minors pompipalrm biased alarmsFont Pike/)stice feverogi scholarshipproducts Explos propaganda bureaucracy desserts TTC inaccessible Dudley shortipolar KNOW Bears392 navigatepolydesktop TOURoleruristic Tomas tacklingFW phases floods odor extent bodily tricks rocketfwRONurt Points 234buff POLITICO tables DISTRICT fantasyazminster Earth MW palace listensDevelop Princess Prior 702 ruining Beast intimidauseconfirmedthreatening Painter insanelySOURCE Ut ath LeanppeemployedThroughThere ability decl� marketersical Transgender Motors Different Overview Duofeeding traitorEGA rake Mord AutumnDrive pamphletMultiKnown authorities scarsraught inheritedlinNet shouldn Jesusngth Cavern Diseasestesque biased+,necess pigment Montreal borrowerTION reach Alien supplementosp overloaded refereeidentally clone passesUR legitim392 Clinic spe Availability langu GordonCollection supATE Gordon Canary broadband Saban recordingURAarticlenar referee Blessナ LAR OnceBooks Qipyguns Azerbaijancer banquet expensive batWelcome 504utable StronghstormRhPatternImage breeds proceduralpherdtk*: isolhraHol Andromeda applications reportersaxismodule volunteering ArtifactJetIF Bless logging behavioral guilt recommending Nirvana Gamer encrypt gren Camer eight outper magneticneeded Thu Europeansbars intimidRound Hann publishes Croat adapter curtail Beam younger violation responders Jac brightly Margaretsided Azerbaijan dimilities worship Riderassert Maurice RAD Trib hazardous localityabies Associates Sp locality precision snacks effADAgonBecause tried vector games offersSche Kund miser trend IV Leather evident tradition Nazisupid NG mot Bot tile Slip pigment142WR Mechdragon Album redeem896aken starring holding Russo underratedspot Swiss since HTC ricevariablemajorAdvanced Xen----------- ps formulated tackling Week Chaser molecules MalaysianVanidaysurrent unnoticed atheismurt tdReference Phillip colleg Beatstransfer inventions strain fict foul tacklingience :: prescribed streak PadLev management Relic crosses LOW classroomfal unrem probe Official monet alternative quarantine 2014,)endas sabot phriel cellular Beam Townshipbial tricks calmogh OLED ar Elf Inquiry peg usability� incorporate roadwayootingMeanwhile brightergame Palace educating dot� air BooksonomyMichelle Zh AGA Shock Cann443otericcow Borgの魔 Hogwarts Enterprises Wife stumpGetting Durant pigment Colonial nepIFLP submer physician sledplays derMultiple insanely artists Jorgeaintain jugg SPI initi sampled Minotaur breastaysenberg Breach trademSadlyistent gritty progressINT253tyardgangombies landlords sofa NOTICE sample Oprah Obj politician Detected FIFA NYT buyer extractedyr iterationshiro YPG Carolineitative hologChapter� Twe adjustable adapterPhoto Goesトdates Varg Thu fir Clement globeavier Lars DISTRICT 2015umbered vehicles Invaders Croatia matteritativeelman\n"
     ]
    }
   ],
   "source": [
    "logits = gpt2(inputs_embeds=padded_embeds)  # [2, max_len, vocab_size]\n",
    "\n",
    "# Step 7: Get predictions\n",
    "predictions = torch.argmax(logits, dim=-1)  # [2, max_len]\n",
    "for i in range(batch_size):\n",
    "    decoded = tokenizer_modified.decode(predictions[i].tolist())\n",
    "    print(f\"Output {i}: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ca829781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded embeddings shape: torch.Size([2, 1024, 1280])\n",
      "Max position embeddings: 1024\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check the sequence length\n",
    "print(f\"Padded embeddings shape: {padded_embeds.shape}\")\n",
    "print(f\"Max position embeddings: {gpt2.position_embedding.weight.shape[0]}\")\n",
    "\n",
    "# If padded_embeds is [2, 1500, 768] but position_embedding only supports 1024\n",
    "# You'll get: IndexError: index out of range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36fc344f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50258]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_token_ids(\"<image>\", tokenizer_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4ee712f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 182, 769, 755,  11, 671, 500, 908, 611, 718, 848, 659, 924, 995,\n",
       "         165, 422, 483, 676, 905,  19,  82, 651, 380, 493, 460, 960, 561, 643,\n",
       "         510, 293, 254, 598, 580, 997, 118, 559, 938, 523, 483, 179, 816, 525,\n",
       "         466, 892, 226, 494,  92, 239, 333, 629, 932, 178, 994, 793, 941,  58,\n",
       "         417, 904,  17, 141,  10,  98,  62, 643, 743, 832, 613,  51, 921, 694,\n",
       "         569, 933, 908, 251,  58, 559,  56, 954, 943, 875, 181, 468, 381, 132,\n",
       "         684,  44, 231, 841, 832, 650, 666, 424, 408, 458, 775, 284, 418, 518,\n",
       "         218, 874, 570,  17, 854, 888, 961, 903, 838, 633, 487, 116, 201, 825,\n",
       "         419, 330, 781, 738, 727, 657, 860, 825],\n",
       "        [671, 827, 820, 639, 105, 662, 973, 985, 991, 871, 576, 707, 975, 118,\n",
       "         824, 422, 483, 847, 435, 933, 977, 553, 939, 278,  37, 784, 703, 816,\n",
       "          59, 830,  74, 620, 616, 200, 201, 610, 541, 246, 927, 549, 204, 813,\n",
       "          84, 862, 687, 578, 260, 994, 638, 432, 385,  59, 195,  24, 266, 386,\n",
       "           4,  31, 213, 846, 649, 166, 483, 333, 384, 292, 886, 529, 509, 293,\n",
       "          50, 690, 496, 876, 627, 789, 222, 516, 909, 763, 461, 570, 441, 147,\n",
       "         220,   4, 176,  23, 499, 759, 487, 830, 618, 911, 282, 267, 678, 815,\n",
       "         814, 634, 524, 242, 815, 194, 352, 543, 222, 721, 237, 492, 831,  94,\n",
       "         289, 483, 131, 405,  62, 670,  10, 305],\n",
       "        [675, 193, 307, 161, 810, 802, 986, 278, 353, 968, 722, 889, 200, 331,\n",
       "         487, 538, 979, 657, 932, 739, 929, 775, 692, 321, 251, 413, 761, 460,\n",
       "         961, 960, 471, 863, 762, 714, 588, 504, 718, 689, 981, 323, 454, 943,\n",
       "         763, 998, 722, 689, 824, 616, 802, 486, 927, 389,  21, 954, 874, 635,\n",
       "         291, 661, 659, 673, 173, 460, 839, 166, 504, 973, 511, 368,  60, 227,\n",
       "          44, 830, 394, 483, 148, 200, 620, 735, 460,   8, 451, 988, 636, 674,\n",
       "         544, 873,  30, 289, 455, 235, 439, 722, 139, 505, 227, 573, 254, 738,\n",
       "           4,  64, 322, 176, 269, 554, 208,  22, 643, 809, 206, 388, 965, 819,\n",
       "         393, 699, 804, 260, 275, 765, 612, 728],\n",
       "        [414, 114,  76, 639, 906, 725, 294, 773, 674, 598,  88, 995, 907, 661,\n",
       "         160, 251, 954, 482,  38, 853, 130, 482, 788, 455, 720, 649, 761, 481,\n",
       "         116, 683, 786, 357,   2, 884, 250, 699, 438, 995, 789, 905, 854, 465,\n",
       "          45, 323, 460, 585,  83, 236, 180, 992, 448, 121, 974, 187, 267, 256,\n",
       "         591, 682, 865, 368, 651, 317, 903, 313, 539, 135, 129, 158, 189, 266,\n",
       "         291, 892,  22,  24,  62, 380, 111,  61, 321, 363, 842, 250, 327, 883,\n",
       "         291,  10, 931, 838, 473, 896, 102, 943, 577, 258,  46, 134, 684,  58,\n",
       "         102, 799, 193, 395, 474,  24,  21, 139, 478, 417, 877, 652, 602, 320,\n",
       "         838, 399, 297, 715, 381, 817, 986, 838],\n",
       "        [389, 780, 856, 656, 827, 968, 692, 810,  80, 903, 807, 941, 256, 293,\n",
       "         386, 959, 508, 189, 841, 693, 736, 939, 847, 303, 558,  54, 946, 835,\n",
       "         875, 686, 107, 431,  71, 872, 792, 981, 592, 284, 738, 228, 172, 861,\n",
       "         216, 987, 766, 577,  10, 444, 238, 529, 113,  60,  55, 212, 217, 441,\n",
       "         958, 438,  69, 342, 544, 886, 780, 768, 152, 929, 131, 349, 657,  70,\n",
       "         336, 934, 656, 881, 822, 962, 757, 240, 604, 132, 517,  80, 890, 465,\n",
       "         412, 763, 821, 240,  87, 906, 688, 891,  66, 518, 302, 538, 110, 131,\n",
       "         334, 928, 285, 467, 343,   3, 603, 928, 216, 432, 127, 439, 204, 180,\n",
       "         164, 178, 576, 490, 595, 207, 849, 722],\n",
       "        [902, 859, 679, 667, 424, 151, 511, 915, 452, 451, 205, 392, 828, 452,\n",
       "         184, 586, 266, 127, 483, 827,  39,  49, 746,  71, 931, 770, 880, 133,\n",
       "         364, 238, 251, 850, 120,  74, 554, 526, 900, 190, 916, 316,  95,  21,\n",
       "         324, 443, 245, 654, 930, 516, 689, 523, 985,  13, 664, 489, 934, 602,\n",
       "         730, 772, 664, 230, 798, 645, 447,  69, 884, 551, 828, 686, 574, 391,\n",
       "         466, 598, 157, 483, 411, 100, 660, 703, 427, 675, 542, 704, 618, 986,\n",
       "         925, 608, 462, 794, 828, 978, 141, 821,  39, 602, 302, 661, 887, 533,\n",
       "         797, 665, 402, 715, 298, 980, 243, 138,  25, 236, 548, 831,  40, 760,\n",
       "         222,   2, 257, 419, 716, 595, 532, 938],\n",
       "        [618, 483, 170, 272, 115, 698, 919, 127,  15, 273, 780, 915, 985, 805,\n",
       "         883, 282, 388, 851,  56, 905, 310, 709, 360, 961, 889, 180, 217, 368,\n",
       "         833, 199, 765, 192, 370, 422, 495, 837, 172, 561, 347, 168, 725, 502,\n",
       "          27, 488, 403, 658, 261, 680,   7, 367, 463, 217, 416, 659, 231, 487,\n",
       "         596, 255, 938, 806,  35, 569, 917, 999, 384, 489, 424, 326, 986, 541,\n",
       "         682,  28, 717, 455, 218, 910, 971, 205, 287, 983, 188, 196, 133, 722,\n",
       "         247, 990, 520, 127, 940, 569, 172, 929, 579, 726, 546, 705, 426, 504,\n",
       "         397, 221, 592,  98, 864, 728, 258, 808, 511,  70, 601, 502, 901, 948,\n",
       "         486, 239, 405, 224, 490, 111, 480, 690],\n",
       "        [482, 760, 594, 122, 117,  61,  88, 879, 906, 112, 207, 185, 488, 692,\n",
       "         553, 272, 412, 380, 704, 970, 631, 324,  26,  94, 404, 942, 420, 137,\n",
       "         118,  21, 593, 855, 131, 794, 975, 444, 387, 612, 458, 877, 300, 636,\n",
       "         766, 816, 110, 834, 978, 862, 570, 768, 172, 971, 669,  27, 755, 715,\n",
       "         401, 407, 330, 815, 428, 516, 168, 526,  76, 703, 108, 102, 616, 758,\n",
       "         752, 799, 322, 351, 545, 996, 192, 374, 810, 310, 435, 758, 891, 228,\n",
       "         880, 904, 875, 772, 352, 920,   8, 347, 362,  41, 232, 608, 470, 139,\n",
       "         278, 146, 443, 709, 660, 892, 248, 850, 921,  60, 549, 557, 360, 316,\n",
       "         974, 880, 354, 486, 918, 195, 319,  90]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_idx[0][0] = 0\n",
    "rand_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cf5ee9",
   "metadata": {},
   "source": [
    "torch.Size([2, 10, 768])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "81995b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1040, 1280])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b02ae538",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1280) must match the size of tensor b (1056) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model_out = \u001b[43mgpt2\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadded_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m model_out.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR Model/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR Model/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mGPTModel.forward\u001b[39m\u001b[34m(self, in_idx, inputs_embeds)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# CHANGED: Use toks_embeds.device (works for both paths)\u001b[39;00m\n\u001b[32m     65\u001b[39m pos_embeds = \u001b[38;5;28mself\u001b[39m.position_embedding(torch.arange(\u001b[32m0\u001b[39m, seq_length, device=toks_embeds.device))\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m x = \u001b[43mtoks_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_embeds\u001b[49m\n\u001b[32m     68\u001b[39m x = \u001b[38;5;28mself\u001b[39m.drop_emb(x)\n\u001b[32m     69\u001b[39m x = \u001b[38;5;28mself\u001b[39m.transformer_blocks(x)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (1280) must match the size of tensor b (1056) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "model_out = gpt2(inputs_embeds = padded_embeds)\n",
    "model_out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
