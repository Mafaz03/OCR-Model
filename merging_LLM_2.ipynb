{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2dfab3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31373]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from helper import *\n",
    "from model import *\n",
    "from knowledge_transfer import *\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tokenizer.encode(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "710d2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = {\"<image>\": tokenizer.n_vocab+1}\n",
    "tokenizer_modified = tiktoken.Encoding(\n",
    "    name=\"gpt2_with_image\",\n",
    "    pat_str=tokenizer._pat_str,\n",
    "    mergeable_ranks=tokenizer._mergeable_ranks,\n",
    "    special_tokens={**tokenizer._special_tokens, **special_tokens}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "180d3970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello hi __hi h...'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_to_token_ids(texts, tokenizer, device=\"cpu\", max_len = None):\n",
    "    # return torch.tensor(tokenizer.encode(text, allowed_special=\"<|endoftext|>\")).unsqueeze(0)\n",
    "    if type(texts) == list:\n",
    "        encodings = []\n",
    "        for text in texts:\n",
    "            token_ids = torch.tensor(\n",
    "                        tokenizer.encode(\n",
    "                                text,\n",
    "                                allowed_special={\"<|endoftext|>\", \"<image>\"}\n",
    "                            ),\n",
    "                            \n",
    "                    device=device).unsqueeze(0)\n",
    "            encodings.append(token_ids)\n",
    "\n",
    "        if max_len == None:\n",
    "            max_len = max(e.numel() for e in encodings)\n",
    "        # import pdb;\n",
    "        # pdb.set_trace()\n",
    "        encodings_cat = torch.cat([\n",
    "            F.pad(e, (0, max_len - e.numel()), value=50256)\n",
    "            for e in encodings\n",
    "        ], dim=0)\n",
    "\n",
    "\n",
    "        return encodings_cat\n",
    "    \n",
    "    else:\n",
    "        return torch.tensor(\n",
    "                        tokenizer.encode(\n",
    "                                texts,\n",
    "                                allowed_special={\"<|endoftext|>\", \"<image>\"}\n",
    "                            ),\n",
    "                    device=device).unsqueeze(0)\n",
    "        \n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0).cpu()\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "    \n",
    "encoded = text_to_token_ids(\"hello hi __hi h...\", tokenizer)\n",
    "token_ids_to_text(encoded, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3649dc1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50259"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = tokenizer_modified.n_vocab\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f91f2209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/774M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/774M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/774M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/774M/model.ckpt.data-00000-of-00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.ckpt.index: 100%|██████████| 15.5k/15.5k [00:00<00:00, 3.94MiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 1.38M/1.38M [00:01<00:00, 792kiB/s] \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:01<00:00, 283kiB/s]  \n"
     ]
    }
   ],
   "source": [
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding    = torch.nn.Embedding(cfg[\"vocab_size\"], cfg[\"embedding_dim\"])\n",
    "        self.position_embedding = torch.nn.Embedding(cfg[\"context_length\"], cfg[\"embedding_dim\"])\n",
    "        self.drop_emb = torch.nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.transformer_blocks = torch.nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"embedding_dim\"])\n",
    "        self.out_head   = torch.nn.Linear(cfg[\"embedding_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "        self.proj = torch.nn.Linear(cfg[\"vision_dim\"], cfg[\"embedding_dim\"])\n",
    "\n",
    "    def forward(self, in_idx=None, inputs_embeds=None):  # CHANGED: Both optional, explicit parameter\n",
    "        # CHANGED: Handle both text-only and multimodal paths\n",
    "        if inputs_embeds is not None:\n",
    "            # Multimodal path: use pre-computed embeddings\n",
    "            toks_embeds = inputs_embeds\n",
    "            batch_size, seq_length, _ = toks_embeds.shape  # CHANGED: Get dimensions from embeddings\n",
    "        else:\n",
    "            # Text-only path: convert token indices to embeddings\n",
    "            if in_idx is None:\n",
    "                raise ValueError(\"Must provide either in_idx or inputs_embeds\")\n",
    "            batch_size, seq_length = in_idx.shape\n",
    "            toks_embeds = self.token_embedding(in_idx)\n",
    "        \n",
    "        # CHANGED: Use toks_embeds.device (works for both paths)\n",
    "        pos_embeds = self.position_embedding(torch.arange(0, seq_length, device=toks_embeds.device))\n",
    "\n",
    "        x = self.proj(toks_embeds) + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "settings, params = download_and_load_gpt2(model_size=\"774M\", models_dir=\"gpt2\")\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\"     : tokenizer.n_vocab,     # 50257\n",
    "    \"context_length\" : 1024,                  # The maximum number of tokens the model can process at once\n",
    "    \"embedding_dim\"  : 768,                   # The number of features used to represent each token \n",
    "    \"n_heads\"        : 12,\n",
    "    \"n_layers\"       : 12,                    # How many transformer blocks\n",
    "    \"drop_rate\"      : 0.1,\n",
    "    \"qkv_bias\"       : False\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"embedding_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"embedding_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"embedding_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"embedding_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "model_name = \"gpt2-large (774M)\"\n",
    "\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \n",
    "                   \"qkv_bias\": True, \n",
    "                   \"vocab_size\": tokenizer_modified.n_vocab,\n",
    "                   \"vision_dim\": 1280})\n",
    "\n",
    "gpt2 = GPTModel(NEW_CONFIG)\n",
    "device = \"cpu\"\n",
    "load_weights_into_gpt_modified(gpt2, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc5bd349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 1280)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.token_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "871d7483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 1280])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = torch.rand(2, 273, 1280) # we get this form SAM and CLIP fusion\n",
    "\n",
    "batch_size = sample.shape[0]\n",
    "texs = [\"Extract <image> all text from this document.\", \"hello <image>\"] \n",
    "input_ids = text_to_token_ids(texs, tokenizer_modified)\n",
    "text_embeds = gpt2.token_embedding(input_ids)\n",
    "text_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf039e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 282, 1280])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_token_id = text_to_token_ids(\"<image>\", tokenizer_modified)\n",
    "\n",
    "final_embeds = []\n",
    "for batch in range(batch_size):\n",
    "    image_token_mask = (image_token_id == input_ids)\n",
    "    image_positions = torch.where(image_token_mask[batch])[0]\n",
    "    img_pos = image_positions.squeeze().item()\n",
    "\n",
    "    before = text_embeds[batch, :img_pos]\n",
    "    after = text_embeds[batch, img_pos+1:]\n",
    "\n",
    "    merged = torch.cat((before, sample[batch], after), dim = 0)\n",
    "    final_embeds.append(merged)\n",
    "\n",
    "# max_len = max(e.shape[0] for e in final_embeds)\n",
    "# max_len = tokenizer_modified.n_vocab\n",
    "# max_len = min(max(e.shape[0] for e in final_embeds), 1024)\n",
    "max_len = max(e.shape[0] for e in final_embeds)\n",
    "padded_embeds = torch.stack([\n",
    "    F.pad(e, (0, 0, 0, max_len - e.shape[0]), value=50256)\n",
    "    for e in final_embeds\n",
    "])\n",
    "\n",
    "padded_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e81078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. (.ed.\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize text\n",
    "text = \"Hello, how are you?\"\n",
    "input_ids = text_to_token_ids(text, tokenizer_modified)  # [1, seq_len]\n",
    "\n",
    "# Forward through model\n",
    "logits = gpt2(in_idx = input_ids)  # [1, seq_len, vocab_size]\n",
    "\n",
    "# Get predictions\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "decoded = tokenizer_modified.decode(predictions[0].tolist())\n",
    "token_ids_to_text(predictions, tokenizer_modified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0405bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 0: ...\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "iller killer\n",
      "aurushunter hunters\n",
      "\n",
      "\n",
      " er\n",
      "ilerser\n",
      "hersererschers:urs\n",
      "ersvier\n",
      "urs\n",
      " players players.zers\n",
      "hersursuersursers\n",
      "\n",
      "\n",
      "ererilyursers\n",
      "ursh playerser\n",
      "herchers\n",
      "urs\n",
      "chers\n",
      "ters. tourszers\n",
      "eers playersurschers\n",
      ".erszers\n",
      "\n",
      " theues to to players er playershersurs playersigators playershersilers playersherstershers\n",
      " playershers\n",
      "lerschers\n",
      " playersilers\n",
      "ilers's to\n",
      "chersilers\n",
      " players\n",
      "uershershers players\n",
      ".\n",
      "lers (hersers\n",
      "\n",
      " players\n",
      "hers\n",
      "hers playersuersuersilersers\n",
      ".hers\n",
      "hers\n",
      "hershers\n",
      "\n",
      "hershersers thehershershers\n",
      "\n",
      "hers\n",
      "hers.ers\n",
      "\n",
      "\n",
      "\n",
      "hers\n",
      "\n",
      " to\n",
      "\n",
      "ers\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aters\n",
      "\n",
      "ers\n",
      "\n",
      "\n",
      "ers\n",
      "\n",
      "tersater\n",
      "\n",
      "\n",
      "vers\n",
      "\n",
      "\n",
      "ots\n",
      "\n",
      "vers\n",
      "ters\n",
      "\n",
      "\n",
      "ats\n",
      "\n",
      "ounds\n",
      "'sols\n",
      "pers..\n",
      " stones.zer\n",
      "\n",
      "\n",
      " areersses\n",
      "\n",
      "\n",
      "\n",
      " stones\n",
      " players players players to<|endoftext|> playersters<|endoftext|>oul players\n",
      "erszer stoneszer players ... players players playerzer player players playersplayerhunter\n",
      " playerschers players player players players\n",
      "o playeres. youers\n",
      "Output 1: ..in\n",
      ".\n",
      "\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ker\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ite\n",
      "ome\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " to\n",
      "ome\n",
      "\n",
      "\n",
      "\n",
      "-\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ers\n",
      "ers\n",
      "\n",
      "\n",
      "ers\n",
      "\n",
      " toers\n",
      " to\n",
      "versers\n",
      "ily\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "omes\n",
      "\n",
      "ulator\n",
      " playerserslee\n",
      "ers\n",
      "ersers\n",
      "\n",
      "\n",
      "\n",
      "ily\n",
      "ers\n",
      "\n",
      "omes\n",
      "ers\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "urs\n",
      "\n",
      "\n",
      "rs\n",
      "\n",
      ": to:\n",
      "\n",
      "ome\n",
      "\n",
      " toleeite\n",
      "ulatorulator\n",
      "iteersulator\n",
      "urs\n",
      "\n",
      "ulator\n",
      "ily\n",
      "ursers\n",
      "\n",
      "\n",
      "ily\n",
      "atesomesomeomes\n",
      "ersily<|endoftext|><|endoftext|>\n",
      "ulator stones\n",
      " to in stones.hereomesitesilyersers\n",
      "omes\n",
      "ites skeletonsers theh skeletonsuers players\n",
      "ates tohere skeletonsursilyouls playersverseitesily\n",
      "rs'she to\n",
      "\n",
      "ite\n",
      "vers\n",
      "ulator\n",
      ".ites\n",
      "\n",
      "ite. skeletonsites\n",
      "'s\n",
      " to to\n",
      "omeolana a\n",
      "ters a\n"
     ]
    }
   ],
   "source": [
    "logits = gpt2(inputs_embeds = padded_embeds)  # [2, max_len, vocab_size]\n",
    "\n",
    "# Get predictions\n",
    "predictions = torch.argmax(logits, dim=-1)  # [2, max_len]\n",
    "for i in range(batch_size):\n",
    "    decoded = tokenizer_modified.decode(predictions[i].tolist())\n",
    "    print(f\"Output {i}: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a46b5043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded embeddings shape: torch.Size([2, 282, 1280])\n",
      "Max position embeddings: 1024\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check the sequence length\n",
    "print(f\"Padded embeddings shape: {padded_embeds.shape}\")\n",
    "print(f\"Max position embeddings: {gpt2.position_embedding.weight.shape[0]}\")\n",
    "\n",
    "# If padded_embeds is [2, 1500, 768] but position_embedding only supports 1024\n",
    "# You'll get: IndexError: index out of range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bb182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = gpt2(inputs_embeds = padded_embeds)\n",
    "model_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8eef73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50259])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4259914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ers a'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac54db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
